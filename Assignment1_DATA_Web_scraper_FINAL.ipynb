{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAyiopFATlHc"
      },
      "source": [
        "scrape instagram and facebook / reddit, yahoo answers, hestenettet for statements on pain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yh0wKjaTlIA",
        "outputId": "322b6d8f-5665-4c61-a63b-f9198a8d6b8b"
      },
      "outputs": [],
      "source": [
        "pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOTm1z65ERj0"
      },
      "source": [
        "TEXT GENERATION MODEL: https://huggingface.co/philschmid/flan-t5-base-samsum?text=hej"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY2npwupuGSE"
      },
      "source": [
        "For this project, I am gonna use the Reddit API Praw. I am following this guide on how to implement it and parts of the code will be taken from here. https://towardsdatascience.com/scraping-reddit-data-1c0af3040768"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q6UAxjwbj9O",
        "outputId": "ce8e315c-d8b3-4fe7-b10c-f1cdee1500fa"
      },
      "outputs": [],
      "source": [
        "pip install psaw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6QSc-W7wIbR",
        "outputId": "dd19e413-0588-4ecf-f66c-540250fe71cb"
      },
      "outputs": [],
      "source": [
        "pip install praw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz8_mVPZTlId"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries \n",
        "import praw\n",
        "from psaw import PushshiftAPI\n",
        "import bs4\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from html.parser import HTMLParser\n",
        "from praw.models import MoreComments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjYpMl__wXcC"
      },
      "outputs": [],
      "source": [
        "# authenticating myself!\n",
        "reddit = praw.Reddit(client_id='Qbcp7XRs2X_HWLnshuyAAw',client_secret='yw9xGdrzGOn9wzE1qhVTZrPiAVa-5Q',user_agent='webscraper')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0Qs1EkYnev0"
      },
      "source": [
        "Now to the webscraping. \n",
        "I wanna make sure that the comments and subcomments I get with the keywords are actually about physical and mental pain, so I am limiting my search to a few subreddits that I chose manually using the search function on Reddit.\n",
        "\n",
        "Part of the code is taken from:  https://praw.readthedocs.io/en/latest/tutorials/comments.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwcozieVhpzA",
        "outputId": "cb088864-6bb1-47bf-e204-6dea0688f323"
      },
      "outputs": [],
      "source": [
        "# list of key words\n",
        "words_pain = ['pain','illnes','symptom','sick','treatment','diagnosed','undiagnosed','diagnosis','syndrome','drugs','drug','medication','meds','painkillers','disability','disease']\n",
        "\n",
        "# list to hold scraped posts \n",
        "pain_all=[]\n",
        "\n",
        "def get_comments():      \n",
        "  for submission in reddit.subreddit('ChronicPain+ChronicIllness+backpain+disability+migraine+chronicpainrelief+kneepain+Fibromyalgia+painhub+painmed').hot(limit=None):          # iterating through subreddits\n",
        "    submission.comments.replace_more(limit=None)          # iterate through all the comments \n",
        "    comment_queue = submission.comments[:]               # Seed with top-level\n",
        "    id=submission.id\n",
        "    title=str(submission.title)\n",
        "    for word in words_pain:                              # iterate through keywords    \n",
        "     if word in title or word in submission.comments:    # check if comment or title contains keywords \n",
        "      while comment_queue:\n",
        "        comment = comment_queue.pop(0)\n",
        "        #print(comment.body)\n",
        "        #comment_queue.extend(comment.replies) \n",
        "        # get both titles and bodies \n",
        "        pain_all.append(title)\n",
        "        pain_all.append(comment.body)\n",
        "  return pain_all\n",
        "\n",
        "get_comments()\n",
        "print(len(pain_all))\n",
        "print(pain_all)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fvVjYhneLBT",
        "outputId": "a087b228-9705-493e-a89a-16eaa9467487"
      },
      "outputs": [],
      "source": [
        "print('scraped content: ' ,pain_all)\n",
        "print('amount of scraped content: ',len(pain_all))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cYRwqQjo3qn"
      },
      "source": [
        "# DATA WRANGLING\n",
        "Now we have a list of 2764 posts scraped from 10 different subreddits. \n",
        "\n",
        "As I want to be able to use this dataset for a variety of things; for finetuning and as inputs to different AI models, I want it to be cleaned up and labelled, but I still want to keep the essence intact, that meaning all emojis and special characters aswell as it being case-sensitive. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuKnFPFGo_2P",
        "outputId": "9ec467b0-0f40-4e08-beab-86ddeeb2b4da"
      },
      "outputs": [],
      "source": [
        "import nltk \n",
        "from nltk.tokenize import sent_tokenize\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMYb1rpLoizv",
        "outputId": "c9c25f3e-9c42-4781-f7cb-e1e02deb0aa6"
      },
      "outputs": [],
      "source": [
        "# remove duplicates from list \n",
        "pain = [x for k,x in enumerate(pain_all) if x not in pain_all[:k]]\n",
        "\n",
        "print('content with removed duplicates: ' ,pain)\n",
        "print('number of comments: ',len(pain))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eJ0zpF_yCKP"
      },
      "outputs": [],
      "source": [
        "# hold cleaned dataset \n",
        "pain_newlines = []\n",
        "\n",
        "# remove new lines \n",
        "for item in pain:\n",
        "  pain_newlines.append(item.replace('\\n',''))    \n",
        "\n",
        "# hold cleaned dataset \n",
        "pain_clean = []\n",
        "\n",
        "for item in pain_newlines:\n",
        "  if 'http' in item: \n",
        "    continue\n",
        "  else: \n",
        "    pain_clean.append(item)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n7gLMNhgpGc",
        "outputId": "1fdbf9f1-a9ac-4be1-e25a-42d3293f39c7"
      },
      "outputs": [],
      "source": [
        "# label datasets \n",
        "\n",
        "pain_all=[]\n",
        "\n",
        "# iterate over data to get text elements and index elements \n",
        "for index,text in enumerate(pain_clean):\n",
        "  dictionary_all = {'text':text,'index':index}\n",
        "  pain_all.append(dictionary_all)\n",
        "\n",
        "# words that indicate a comment seeking advice\n",
        "advice_seeking_words = ['do','how','?','which','what','who','when','whom','whose']\n",
        "\n",
        "# hold the labeled data\n",
        "data_labeled = []\n",
        "\n",
        "# iterate over data \n",
        "for item in pain_all:\n",
        "  text = item['text']\n",
        "  tokens = nltk.word_tokenize(text)   \n",
        "  pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "  if any([word in ['do','how','?','which','what','who','when','whom','whose'] for word,pos in pos_tags]):\n",
        "   label = 1     # asking for advie\n",
        "  else: \n",
        "    label = 0    # giving advice\n",
        "  data_labeled.append({'text':text,'label':label})\n",
        "\n",
        "len(data_labeled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeq0DtYWXEpV",
        "outputId": "66c3a91a-c346-4eb0-8acb-f94d56555d97"
      },
      "outputs": [],
      "source": [
        "# split into train and test \n",
        "pain_train = []\n",
        "pain_test = []\n",
        "\n",
        "pain_train.append(data_labeled[:1200])\n",
        "pain_test.append(data_labeled[1200:])\n",
        "\n",
        "# content is in 2d array - needs to be 1d \n",
        "pain_test = pain_test[0]\n",
        "pain_train = pain_train[0]\n",
        "\n",
        "print('test_len: ',len(pain_test))\n",
        "print('train_len: ',len(pain_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFN6pl2Q_pZu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# save data files\n",
        "with open('data_pain_train.json','w',encoding='utf-8') as fout_train:   \n",
        "  #fout.write(str(pain))\n",
        "  json.dump(pain_train,fout_train)\n",
        "\n",
        "with open('data_pain_test.json','w',encoding='utf-8') as fout_test: \n",
        "  json.dump(pain_test,fout_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amyiuDeOfaQY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "datascience",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "b7c2af71d8630b70f7aa2b02182fea5523df0abf4b12c9d16ad6f17eb01b8450"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
